{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "NLP.ipynb",
      "authorship_tag": "ABX9TyNrLzh3BL4kN8Ph5uZsA01F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranaya14/CSA0716/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = PorterStemmer()\n",
        "sentences = [ \"I am running in the park\",\n",
        "             \"the running shoes are on sales\",\n",
        "              \"she ran to catch the busses\"]\n",
        "for sentence in sentences:\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  stemmed_words = [stemmer.stem(word) for word in words ]\n",
        "  stemmed_sentence = \" \".join(stemmed_words)\n",
        "  print(f\"original: {sentence}\")\n",
        "  print(f\"stemmed: {stemmed_sentence}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSqryS1jpjvD",
        "outputId": "752c1ca0-0760-4c07-e7d4-031592a36dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: I am running in the park\n",
            "stemmed: i am run in the park\n",
            "\n",
            "original: the running shoes are on sales\n",
            "stemmed: the run shoe are on sale\n",
            "\n",
            "original: she ran to catch the busses\n",
            "stemmed: she ran to catch the buss\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL2J0GY1qMat",
        "outputId": "fc151919-40f4-4fe4-cf6d-4cb2d8362234"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The house eats small.\n",
            "He reads smart house slowly.\n",
            "I sleeps quick house.\n",
            "The dog sleeps quick.\n",
            "John sleeps quick dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define sentence patterns\n",
        "sentence_patterns = [\n",
        "    \"The {noun} {verb} {adjective}.\",\n",
        "    \"{Noun} {verb} {adjective} {noun}.\",\n",
        "    \"I {verb} {adjective} {noun}.\",\n",
        "    \"She {verb} {noun}.\",\n",
        "    \"He {verb} {adjective} {noun} {adverb}.\",\n",
        "]\n",
        "\n",
        "# Define vocabulary\n",
        "nouns = [\"cat\", \"dog\", \"ball\", \"house\", \"book\"]\n",
        "verbs = [\"runs\", \"jumps\", \"sleeps\", \"reads\", \"eats\"]\n",
        "adjectives = [\"quick\", \"lazy\", \"smart\", \"tall\", \"small\"]\n",
        "adverbs = [\"slowly\", \"quickly\", \"loudly\", \"silently\"]\n",
        "Noun = \"John\"\n",
        "\n",
        "# Function to construct a random sentence\n",
        "def construct_sentence():\n",
        "    pattern = random.choice(sentence_patterns)\n",
        "    sentence = pattern.format(\n",
        "        noun=random.choice(nouns),\n",
        "        verb=random.choice(verbs),\n",
        "        adjective=random.choice(adjectives),\n",
        "        adverb=random.choice(adverbs),\n",
        "        Noun=Noun,\n",
        "    )\n",
        "    return sentence\n",
        "\n",
        "# Generate and print random sentences\n",
        "for _ in range(5):\n",
        "    sentence = construct_sentence()\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4rAxGeoqWGs",
        "outputId": "86267199-7d44-4f21-eaa5-5d64549b0aa6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John runs quick dog.\n",
            "He runs smart ball quickly.\n",
            "She eats book.\n",
            "John reads lazy ball.\n",
            "The house sleeps tall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tagged = pos_tag(word_tokenize(sample_text))\n",
        "chunker = RegexpParser(\"\"\"\n",
        "\t\t\t\t\tNP: {<DT>?<JJ>*<NN>}\n",
        "\t\t\t\t\tP: {<IN>}\n",
        "\t\t\t\t\tV: {<V.*>}\n",
        "\t\t\t\t\tPP: {<p> <NP>}\n",
        "\t\t\t\t\tVP: {<V> <NP|PP>*}\n",
        "          \"\"\")\n",
        "output = chunker.parse(tagged)\n",
        "print(\"After Extracting\\n\", output)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcoj96qXq6Mj",
        "outputId": "15a251d1-edee-491d-c29b-b45d0643124d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Extracting\n",
            " (S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  (VP (V jumps/VBZ))\n",
            "  (P over/IN)\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "text = \"This is one simple example.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokens, tagset = \"universal\")\n"
      ],
      "metadata": {
        "id": "IPJB3Y9hxpnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
        "\t\"Sukanya is getting married next year. \" \\\n",
        "\t\"Marriage is a big step in one’s life.\" \\\n",
        "\t\"It is both exciting and frightening. \" \\\n",
        "\t\"But friendship is a sacred bond between people.\" \\\n",
        "\t\"It is a special kind of love between us. \" \\\n",
        "\t\"Many of you must have tried searching for a friend \"\\\n",
        "\t\"but never found the right one.\"\n",
        "\n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        "\n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "\n",
        "\t# Word tokenizers is used to find the words\n",
        "\t# and punctuation in a string\n",
        "\twordsList = nltk.word_tokenize(i)\n",
        "\n",
        "\t# removing stop words from wordList\n",
        "\twordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "\t# Using a Tagger. Which is part-of-speech\n",
        "\t# tagger or POS-tagger.\n",
        "\ttagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "\tprint(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_9l6xxVzpYF",
        "outputId": "2ba1db00-174e-48bc-d9eb-b6ee22825d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
            "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
            "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life.It', 'NN'), ('exciting', 'VBG'), ('frightening', 'NN'), ('.', '.')]\n",
            "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people.It', 'NN'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading : Package '' not found in index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "txt = \"your shirt collection in fantastic. \"\\\n",
        "      \" is everything okay .\" \\\n",
        "      \"are you fine .\"\n",
        "\n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "\twordsList = nltk.word_tokenize(i)\n",
        "\twordsList = [w for w in wordsList if not w in stop_words]\n",
        "\ttagged = nltk.pos_tag(wordsList)\n",
        "\tprint(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJtft7Mw2fTf",
        "outputId": "e6c44443-f0ab-48d6-f25a-83939aab20fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('shirt', 'NN'), ('collection', 'NN'), ('fantastic', 'NN'), ('.', '.')]\n",
            "[('everything', 'NN'), ('okay', 'PRP'), ('.are', 'JJ'), ('fine', 'JJ'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "brown.words()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMAIcGOG0s7V",
        "outputId": "9097c27e-d999-43c8-a366-4cb19086652a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "text = \"Natural language processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj8M8VRa31kG",
        "outputId": "1c7bfede-0978-4fe1-9a26-0dce0d257e39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Natural language processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
            "\n",
            "POS Tags:\n",
            "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('through', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy language model (you can use a larger model for better accuracy)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define two example sentences\n",
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"A fast brown fox leaps over a sleepy dog.\"\n",
        "\n",
        "# Process the sentences with spaCy to obtain Doc objects\n",
        "doc1 = nlp(sentence1)\n",
        "doc2 = nlp(sentence2)\n",
        "\n",
        "# Calculate the semantic similarity between the two sentences\n",
        "semantic_similarity = doc1.similarity(doc2)\n",
        "\n",
        "# Print the semantic similarity score\n",
        "print(f\"Semantic Similarity Score: {semantic_similarity:.2f}\")\n"
      ],
      "metadata": {
        "id": "h2NO5VLi5FWx",
        "outputId": "eb0050e6-e596-4ad1-de76-7ddbbd50764a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Similarity Score: 0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-eba4de163b74>:15: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  semantic_similarity = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download NLTK data (only needs to be done once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a Sentiment Intensity Analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Sample sentences for sentiment analysis\n",
        "sentences = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The weather is terrible today.\",\n",
        "    \"The movie was okay, but not great.\",\n",
        "    \"I feel neutral about this.\",\n",
        "]\n",
        "\n",
        "# Analyze sentiment for each sentence\n",
        "for sentence in sentences:\n",
        "    sentiment_scores = sia.polarity_scores(sentence)\n",
        "    compound_score = sentiment_scores['compound']\n",
        "\n",
        "    if compound_score >= 0.05:\n",
        "        sentiment = \"Positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Sentiment: {sentiment} (Compound Score: {compound_score:.2f})\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW72-31X6NRq",
        "outputId": "940c04dd-2b4e-4f94-f7ec-48d8ec55860d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I love this product! It's amazing.\n",
            "Sentiment: Positive (Compound Score: 0.85)\n",
            "\n",
            "Sentence: The weather is terrible today.\n",
            "Sentiment: Negative (Compound Score: -0.48)\n",
            "\n",
            "Sentence: The movie was okay, but not great.\n",
            "Sentiment: Negative (Compound Score: -0.61)\n",
            "\n",
            "Sentence: I feel neutral about this.\n",
            "Sentiment: Neutral (Compound Score: 0.00)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}